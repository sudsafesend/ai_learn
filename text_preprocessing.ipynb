{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Model Building\n",
    "1. Data Ingestion\n",
    "2. Data Preprocessing\n",
    "2.1 Cleaning\n",
    "Lowercasing\n",
    "Remove HTML\n",
    "Remove URLs\n",
    "Remove stopwords\n",
    "Stemming/Lemmatization (finding root word)\n",
    "Tokenization\n",
    "Remove special characters\n",
    "Remove white spaces\n",
    "2.2 Part of Speech (POS) Tagging and Chunking\n",
    "POS tagging\n",
    "Chunking\n",
    "Emoji removal\n",
    "2.3 Encoding\n",
    "One-Hot Encoding\n",
    "Label Encoding\n",
    "TF-IDF\n",
    "2.4 Embedding\n",
    "Word2Vec\n",
    "3. Model Building\n",
    "4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22686</th>\n",
       "      <td>The China Syndrome is a perfectly paced thrill...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20830</th>\n",
       "      <td>Jennifer Jason Leigh and Mare Winningham are a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27969</th>\n",
       "      <td>Nowadays it is sort of a trend to look upon al...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25063</th>\n",
       "      <td>I am not a fan of musicals, but I am a huge fa...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7256</th>\n",
       "      <td>For Romance's sake, as a married man. The foll...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "22686  The China Syndrome is a perfectly paced thrill...  positive\n",
       "20830  Jennifer Jason Leigh and Mare Winningham are a...  negative\n",
       "27969  Nowadays it is sort of a trend to look upon al...  negative\n",
       "25063  I am not a fan of musicals, but I am a huge fa...  positive\n",
       "7256   For Romance's sake, as a married man. The foll...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"review\"][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove chat jargons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one reviewer mentioned watching 1 oz episode y...\n",
      "1        wonderful little production filming technique ...\n",
      "2        thought wonderful way spend time hot summer we...\n",
      "3        basically family little boy jake think zombie ...\n",
      "4        petter matteis love time money visually stunni...\n",
      "                               ...                        \n",
      "49995    thought movie right good job wasnt creative or...\n",
      "49996    bad plot bad dialogue bad acting idiotic direc...\n",
      "49997    catholic taught parochial elementary school nu...\n",
      "49998    im going disagree previous comment side maltin...\n",
      "49999    one expects star trek movie high art fan expec...\n",
      "Name: review, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Ensure stopwords are downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Use when we want to reduce the word to its base form, accuracy is more\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "chat_words={\n",
    "    \"AFAIK\":\"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\":\"As Soon As Possible\",\n",
    "    \"BTW\":\"By The Way\",\n",
    "    \"B4\":\"Before\",\n",
    "    \"LAMO\":\"Laugh My A.. Off\",\n",
    "    \"FYI\":\"For your information\"\n",
    "}\n",
    "\n",
    "def normalize_chat_slang(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def clean_up(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = emoji.demojize(text)  # Convert emojis to text\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = normalize_chat_slang(text)  # Convert chat jargon\n",
    "    text = lemmatize_text(text)  # Lemmatize the text\n",
    "    text = remove_stopwords(text)  # Remove stopwords\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Apply the clean_up function to the \"review\" column\n",
    "data[\"review\"] = data[\"review\"].apply(lambda x: clean_up(str(x)))\n",
    "print(data[\"review\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is david\n"
     ]
    }
   ],
   "source": [
    "# Auto correct spelling\n",
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spellings(text):\n",
    "    return (TextBlob(text)).correct().string\n",
    "print(correct_spellings(\"my namee is david\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#ignore stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "data[\"review\"] = data[\"review\"].apply(lambda x: remove_stopwords(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 14.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 15.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "['Hello', ',', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "# tokenise using spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download the model if not already installed\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "#Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenize_text(text)\n",
    "print(tokens)\n",
    "\n",
    "#['Hello', ',', 'how', 'are', 'you', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SudhindraGarre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized: running ran run\n",
      "Stemmed: run ran run\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization/Stemming using nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Use when we want to reduce the word to its base form, accuracy is more\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "# Use when we want to reduce the word to its root form, accuracy is less\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(stemmed_text)\n",
    "\n",
    "# Example usage\n",
    "text = \"running ran runs\"\n",
    "lemmatized = lemmatize_text(text)\n",
    "stemmed = stem_text(text)\n",
    "\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "print(\"Stemmed:\", stemmed)\n",
    "\n",
    "# Lemmatized: running ran run\n",
    "# Stemmed: run ran run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install transformers dataset tokenizer seqeval -q\n",
    "\n",
    "NER - Named Entity Recognition\n",
    "POS - Part of Speech\n",
    "\n",
    "#Use Hugging face transformer & create pipeline to achieve Data ingestion (with BERT), preprocessing, training, evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "    num_rows: 14041\n",
      "})\n",
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n",
      "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)\n",
      "{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n",
      "{'input_ids': [[101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n",
      "[CLS]___________________________________ -100\n",
      "the_____________________________________ 0\n",
      "european________________________________ 3\n",
      "commission______________________________ 4\n",
      "said____________________________________ 0\n",
      "on______________________________________ 0\n",
      "thursday________________________________ 0\n",
      "it______________________________________ 0\n",
      "disagreed_______________________________ 0\n",
      "with____________________________________ 0\n",
      "german__________________________________ 7\n",
      "advice__________________________________ 0\n",
      "to______________________________________ 0\n",
      "consumers_______________________________ 0\n",
      "to______________________________________ 0\n",
      "shu_____________________________________ 0\n",
      "##n_____________________________________ 0\n",
      "british_________________________________ 7\n",
      "lamb____________________________________ 0\n",
      "until___________________________________ 0\n",
      "scientists______________________________ 0\n",
      "determine_______________________________ 0\n",
      "whether_________________________________ 0\n",
      "mad_____________________________________ 0\n",
      "cow_____________________________________ 0\n",
      "disease_________________________________ 0\n",
      "can_____________________________________ 0\n",
      "be______________________________________ 0\n",
      "transmitted_____________________________ 0\n",
      "to______________________________________ 0\n",
      "sheep___________________________________ 0\n",
      "._______________________________________ 0\n",
      "[SEP]___________________________________ -100\n",
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0], 'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\SudhindraGarre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     89\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m---> 93\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-ner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     95\u001b[0m evaluation_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     96\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m     97\u001b[0m per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     98\u001b[0m per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m     99\u001b[0m num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    100\u001b[0m weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m    101\u001b[0m )\n",
      "File \u001b[1;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\SudhindraGarre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1808\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1806\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\SudhindraGarre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2344\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2341\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2343\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32mc:\\Users\\SudhindraGarre\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\SudhindraGarre\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2214\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2216\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2217\u001b[0m         )\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2219\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "\n",
    "dataset = datasets.load_dataset(\"conll2003\",trust_remote_code=True) # Load the dataset\n",
    "\n",
    "print(dataset['train']) # Print the training set info\n",
    "##  Dataset({\n",
    "#     features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
    "#     num_rows: 14041\n",
    "# })\n",
    "\n",
    "example_text = dataset['train'][0] # Get the first example from the training set\n",
    "print(example_text) # Print the training set info\n",
    "## {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n",
    "\n",
    "print(dataset['train'].features['ner_tags']) # Print the test set info\n",
    "## Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\",trust_remote_code=True) # Load the tokenizer using pretrained lower cased BERT model\n",
    "## example_text['tokens'] = ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
    "\n",
    "tokenised_input = tokenizer(example_text['tokens'], truncation=True, is_split_into_words=True) # Tokenize the input text\n",
    "print(tokenised_input) # Print the tokenized input\n",
    "## tokenised_input = {'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenised_input['input_ids']) # Convert the input ids to tokens\n",
    "print(tokens) # Print the tokens\n",
    "## tokens = ['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n",
    "\n",
    "word_ids = tokenised_input.word_ids() # Get the word ids from the tokenized input\n",
    "print(word_ids) # Print the word ids\n",
    "##[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n",
    "\n",
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "\n",
    "    #tokeinze ids\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set â€“100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "res_tokenized_inputs = tokenize_and_align_labels(dataset['train'][3:4]) # Tokenize and align the labels for the first two examples in the training set\n",
    "print(res_tokenized_inputs) # Print the tokenized inputs\n",
    "##{'input_ids': [[101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n",
    "\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(res_tokenized_inputs[\"input_ids\"][0]),res_tokenized_inputs[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\") # Print the tokens and labels for the first example in the training set\n",
    "\n",
    "# Applying on entire data\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "# Print the tokenized datasets info\n",
    "print(tokenized_datasets[\"train\"][0]) # Print the tokenized datasets info\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\",num_labels=9)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "args = TrainingArguments(\n",
    "\"test-ner\",\n",
    "eval_strategy = \"epoch\",\n",
    "learning_rate=2e-5,\n",
    "per_device_train_batch_size=16,\n",
    "per_device_eval_batch_size=16,\n",
    "num_train_epochs=1,\n",
    "weight_decay=0.01\n",
    ")\n",
    "\n",
    "# print('dataset info :/n',dataset) # Print the dataset info\n",
    "# print('dataset keys :/n',dataset.keys()) # Print the keys of the dataset\n",
    "\n",
    "# Print the training set info\n",
    "# print('train dataset info:',dataset['train'].features) # Print the features of the training set\n",
    "# print('dataset train features keys',dataset['train'].features.keys()) # Print the keys of the features of the training set\n",
    "# print('dataset train features ner_tags',dataset['train'].features['ner_tags']) # Print the ner_tags feature info of the training set\n",
    "# print('dataset train features pos_tags',dataset['train'].features['pos_tags']) # Print the pos_tags feature info of the training set\n",
    "# print('First document/sentence: ',dataset['train'][0]) # Print the tokens feature info of the training set\n",
    "\n",
    "# # Print the test set info\n",
    "# print('test dataset info',dataset['test'].features) # Print the features of the test set\n",
    "# print('dataset test keys',dataset['test'].keys()) # Print the keys of the test set\n",
    "# print('dataset test features',dataset['test'].features) # Print the features of the test set\n",
    "# print('dataset test features keys',dataset['test'].features.keys()) # Print the keys of the features of the test set\n",
    "\n",
    "# # Print the validation set info\n",
    "# print('validation dataset info',dataset['validation'].features) # Print the features of the validation set\n",
    "# print('dataset validation keys',dataset['validation'].keys()) # Print the keys of the validation set\n",
    "# print('dataset validation features',dataset['validation'].features) # Print the features of the validation set\n",
    "# print('dataset validation features keys',dataset['validation'].features.keys()) # Print the keys of the features of the validation set\n",
    "\n",
    "# print(dataset['train'].features['ner_tags']) # Print the ner_tags feature info\n",
    "# print(dataset['train'][0]) # Check the first example in the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sudhindragarre\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/204.1 MB 10.7 MB/s eta 0:00:19\n",
      "    --------------------------------------- 4.7/204.1 MB 11.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 6.3/204.1 MB 10.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 8.7/204.1 MB 10.5 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 11.0/204.1 MB 10.7 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 13.6/204.1 MB 10.8 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 16.0/204.1 MB 11.1 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 18.6/204.1 MB 11.1 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 21.2/204.1 MB 11.2 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 23.6/204.1 MB 11.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 25.7/204.1 MB 11.1 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 27.8/204.1 MB 11.1 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 30.4/204.1 MB 11.1 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 32.5/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 34.6/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 37.2/204.1 MB 11.0 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 39.6/204.1 MB 11.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 41.9/204.1 MB 11.0 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 44.6/204.1 MB 11.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 46.9/204.1 MB 11.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 49.0/204.1 MB 11.1 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 51.9/204.1 MB 11.2 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 54.8/204.1 MB 11.3 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 57.4/204.1 MB 11.3 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 60.0/204.1 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 62.7/204.1 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 65.0/204.1 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 67.4/204.1 MB 11.5 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 69.7/204.1 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 71.8/204.1 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 73.9/204.1 MB 11.4 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 76.0/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 78.1/204.1 MB 11.3 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 80.5/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 82.6/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 85.2/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 87.8/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 90.2/204.1 MB 11.3 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 92.8/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 94.9/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 97.3/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 99.6/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 102.0/204.1 MB 11.3 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 104.1/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 106.4/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 109.1/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 111.4/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 113.8/204.1 MB 11.3 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 115.9/204.1 MB 11.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 118.5/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 120.8/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 123.5/204.1 MB 11.3 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 125.8/204.1 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 128.5/204.1 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 131.3/204.1 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 133.4/204.1 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 136.1/204.1 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 138.7/204.1 MB 11.4 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 141.3/204.1 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 143.9/204.1 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 146.3/204.1 MB 11.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 148.4/204.1 MB 11.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 151.0/204.1 MB 11.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 153.6/204.1 MB 11.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 156.5/204.1 MB 11.4 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 158.9/204.1 MB 11.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 161.2/204.1 MB 11.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 163.8/204.1 MB 11.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 166.5/204.1 MB 11.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 168.8/204.1 MB 11.4 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 171.4/204.1 MB 11.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.1/204.1 MB 11.5 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 176.4/204.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 179.3/204.1 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 181.7/204.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 184.3/204.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 186.6/204.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 188.7/204.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.1/204.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 193.7/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.3/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.0/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.3/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "tensor([[0.2240, 0.2814, 0.1664],\n",
      "        [0.6816, 0.5201, 0.5724],\n",
      "        [0.7752, 0.1798, 0.9105],\n",
      "        [0.7982, 0.1467, 0.1631],\n",
      "        [0.3438, 0.1211, 0.4845]])\n"
     ]
    }
   ],
   "source": [
    "# Install torch if not already installed\n",
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
